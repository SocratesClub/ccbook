---
interact_link: content/09-08-random-forests.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Random Forests
prev_page:
  url: /09-07-support-vector-machines.html
  title: |-
    Support Vector Machines
next_page:
  url: /09-09-machine-learning-summary.html
  title: |-
    Summary
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<main class="jupyter-page">

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="In-Depth:-Decision-Trees-and-Random-Forests">In-Depth: Decision Trees and Random Forests<a class="anchor-link" href="#In-Depth:-Decision-Trees-and-Random-Forests"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!--BOOK_INFORMATION-->

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>
<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><!--NAVIGATION-->
&lt; <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> | <a href="Index.ipynb">Contents</a> | <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> &gt;</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Previously</p>
<ul>
<li>simple generative classifier (naive Bayes; see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>) </li>
<li>powerful discriminative classifier (support vector machines; see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>).</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Random-Forests"><em>Random Forests</em><a class="anchor-link" href="#Random-Forests"> </a></h2><ul>
<li>Another powerful &amp; non-parametric algorithm  </li>
<li>Random forests are an example of an <strong>ensemble method</strong>, <ul>
<li>meaning that it relies on aggregating the results of an ensemble of simpler estimators.</li>
</ul>
</li>
</ul>
<p>The sum can be greater than the parts:</p>
<ul>
<li>a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!</li>
</ul>
<p>We will see examples of this in the following sections.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Motivating-Random-Forests:-Decision-Trees">Motivating Random Forests: Decision Trees<a class="anchor-link" href="#Motivating-Random-Forests:-Decision-Trees"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Random forests are an example of an <em>ensemble learner</em> built on decision trees.</p>
<ul>
<li>For this reason we'll start by discussing decision trees.</li>
</ul>
<p>Decision trees are extremely intuitive ways to classify or label objects:</p>
<ul>
<li>you simply ask a series of questions designed to zero-in on the classification.</li>
</ul>
<p>For example, if you wanted to build a decision tree to classify who will be alive after the Titanic disaster, you might construct the one shown here:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Titanic-dataset">Titanic dataset<a class="anchor-link" href="#Titanic-dataset"> </a></h3><p><img src="img/figures/05.08-decision-tree-titanic.png" alt=""></p>
<p><a href="06.00-Figure-Code.ipynb#Decision-Tree-Example">figure source in Appendix</a></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#20114;&#21160;&#28216;&#25103;&#65306;&#29468;&#19968;&#19979;&#36873;&#20102;&#21738;&#19968;&#20010;&#21160;&#29289;&#65311;">&#20114;&#21160;&#28216;&#25103;&#65306;&#29468;&#19968;&#19979;&#36873;&#20102;&#21738;&#19968;&#20010;&#21160;&#29289;&#65311;<a class="anchor-link" href="#&#20114;&#21160;&#28216;&#25103;&#65306;&#29468;&#19968;&#19979;&#36873;&#20102;&#21738;&#19968;&#20010;&#21160;&#29289;&#65311;"> </a></h2><p>é²¸é±¼ğŸ‹ã€è™è ğŸ¦‡ã€çŒ´å­ğŸ’ã€ç¿¼é¾™ğŸ‰ã€çŒ«ğŸˆã€ç‹ç‹¸ğŸ¦Šã€è€è™ğŸ…ã€ä¼é¹…ğŸ§ã€å‡ ç»´é¸ŸğŸ¦ã€çŒ«å¤´é¹°ğŸ¦‰ã€é©¬ğŸ´ã€å±±ç¾ŠğŸã€å­”é›€ğŸ¦šã€é¸¡ğŸ”ã€ç‰›ğŸ‚</p>
<p><img src="img/figures/05.08-decision-tree.png" alt=""></p>
<p><a href="06.00-Figure-Code.ipynb#Decision-Tree-Example">figure source in Appendix</a></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#26696;&#20363;&#20998;&#26512;&#65306;&#20170;&#22825;&#26159;&#21542;&#25171;&#29699;&#9977;&#65311;">&#26696;&#20363;&#20998;&#26512;&#65306;&#20170;&#22825;&#26159;&#21542;&#25171;&#29699;&#9977;&#65311;<a class="anchor-link" href="#&#26696;&#20363;&#20998;&#26512;&#65306;&#20170;&#22825;&#26159;&#21542;&#25171;&#29699;&#9977;&#65311;"> </a></h2><ul>
<li>ä¸€ç»„14å¤©å¤©æ°”æ•°æ®(æŒ‡æ ‡åŒ…æ‹¬outlookï¼Œtemperatureï¼Œhumidityï¼Œwindy)ï¼Œå¹¶å·²çŸ¥è¿™äº›å¤©æ°”æ˜¯å¦æ‰“çƒ(play)ã€‚</li>
<li>å¦‚æœç»™å‡ºæ–°ä¸€å¤©çš„æ°”è±¡æŒ‡æ ‡æ•°æ®:sunny,cool,high,TRUEï¼Œåˆ¤æ–­ä¸€ä¸‹ä¼šä¸ä¼šå»æ‰“çƒã€‚</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;../data/play.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="s1">&#39;yes&#39;</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;play&#39;</span><span class="p">]]</span>
<span class="n">df</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Outlook</th>
      <th>temperature</th>
      <th>humidity</th>
      <th>windy</th>
      <th>play</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Sunny</td>
      <td>hot</td>
      <td>high</td>
      <td>False</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Sunny</td>
      <td>hot</td>
      <td>high</td>
      <td>True</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Overcast</td>
      <td>hot</td>
      <td>high</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Rainy</td>
      <td>mild</td>
      <td>high</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Rainy</td>
      <td>cool</td>
      <td>normal</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Rainy</td>
      <td>cool</td>
      <td>normal</td>
      <td>True</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Overcast</td>
      <td>cool</td>
      <td>normal</td>
      <td>True</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Sunny</td>
      <td>mild</td>
      <td>high</td>
      <td>False</td>
      <td>no</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Sunny</td>
      <td>cool</td>
      <td>normal</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Rainy</td>
      <td>mild</td>
      <td>normal</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Sunny</td>
      <td>mild</td>
      <td>normal</td>
      <td>True</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Overcast</td>
      <td>mild</td>
      <td>high</td>
      <td>True</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Overcast</td>
      <td>hot</td>
      <td>normal</td>
      <td>False</td>
      <td>yes</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Rainy</td>
      <td>mild</td>
      <td>high</td>
      <td>True</td>
      <td>no</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;play&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>yes    9
no     5
Name: play, dtype: int64</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Entropy">Entropy<a class="anchor-link" href="#Entropy"> </a></h3><p>æ ·æœ¬é›†åˆæ€»æ ·æœ¬æ•°ä¸º$D$ï¼Œå…¶ä¸­å…±æœ‰$K$ç±»æ ·æœ¬ï¼Œå…¶ä¸­ç¬¬$k$ç±»æ ·æœ¬æ‰€å æ¯”ä¾‹ä¸º$p_k (k = 1, 2, ...K)$ï¼Œåˆ™$D$ä¸ªæ ·æœ¬çš„ä¿¡æ¯ç†µ$E(D)$ä¸ºï¼š</p>
$$E(D) = \sum_{k=1}^{K} -P_k log_2{P_k}$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>E0 =  -9/14 <em> np.log2(9/14) - 5/14 </em> np.log2(5/14) = 0.9402859586706311</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Information-Gains">Information Gains<a class="anchor-link" href="#Information-Gains"> </a></h3><p>å‡å®šç¦»æ•£å±æ€§$a$æœ‰$V$ä¸ªå¯èƒ½çš„å–å€¼$(a_1, a_2,...a_V)$ã€‚</p>
<ul>
<li>å¦‚æœä½¿ç”¨å±æ€§$a$å¯¹$D$ä¸ªæ ·æœ¬è¿›è¡Œåˆ’åˆ†ï¼Œåˆ™ä¼šäº§ç”Ÿ$V$ä¸ªåˆ†æ”¯èŠ‚ç‚¹ã€‚</li>
<li>å‡è®¾å…¶ä¸­ç¬¬$v$ä¸ªåˆ†æ”¯èŠ‚ç‚¹åŒ…å«$D_v$ä¸ªæ ·æœ¬ï¼Œ</li>
<li>æˆ‘ä»¬å¯ä»¥è®¡ç®—å‡º$D_v$ä¸ªæ ·æœ¬çš„ä¿¡æ¯ç†µ</li>
<li>è€ƒè™‘åˆ°æ ·æœ¬æ•°è¶Šå¤šçš„åˆ†æ”¯èŠ‚ç‚¹å½±å“åŠ›è¶Šå¤§ï¼Œç»™åˆ†æ”¯èŠ‚ç‚¹èµ‹äºˆæƒé‡$D_v/D$</li>
<li>å¯ä»¥è®¡ç®—ä½¿ç”¨å±æ€§$a$å¯¹$D$ä¸ªæ ·æœ¬è¿›è¡Œåˆ’åˆ†å¯ä»¥è·å¾—çš„ä¿¡æ¯å¢ç›Šï¼ˆinformation gainï¼‰:</li>
</ul>
$$Gain(D, a) = Ent(D) - \sum_{v=1}^{V} \frac{D_v}{D} Ent(D_v) $$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;windy&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">&#39;play&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>windy  y
False  0    2
       1    6
True   0    3
       1    3
Name: play, dtype: int64</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>E1 =  -6/8 <em> np.log2(6/8) - 2/8 </em> np.log2(2/8) = 0.8112781244591328</li>
<li>E2 =  -3/6 <em> np.log2(3/6) - 3/6 </em> np.log2(3/6) = 1.0 </li>
<li>Gain (wind) = 0.940 - (8/14) <em> 0.811 - (6/14) </em> 1.0 = 0.048</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;humidity&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">&#39;play&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>humidity  y
high      0    4
          1    3
normal    0    1
          1    6
Name: play, dtype: int64</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>E1 =  -3/7 <em> np.log2(3/7) - 4/7 </em> np.log2(4/7) = 0.9852281360342515</li>
<li>E2 =  -6/7 <em> np.log2(6/7) - 1/7 </em> np.log2(1/7) = 0.5916727785823275</li>
<li>Gain (humidity) = 0.940 - (7/14) <em> 0.985 - (7/14) </em> 0.592 = 0.151</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">freq_list</span><span class="p">):</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">freq_list</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">i</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">freq_list</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">+=</span> <span class="o">-</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> 
    <span class="k">return</span>  <span class="n">e</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">freq_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Outlook&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Sunny&#39;</span><span class="p">][</span><span class="s1">&#39;play&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">entropy</span><span class="p">(</span><span class="n">freq_list</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.9709505944546686</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">information_gain</span><span class="p">(</span><span class="n">feature_name</span><span class="p">,</span> <span class="n">y_name</span><span class="p">):</span>
    <span class="n">freq_list0</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_name</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">freq_list0</span><span class="p">)</span>
    <span class="n">e0</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">freq_list0</span><span class="p">)</span>
    <span class="n">info_gain</span> <span class="o">=</span> <span class="n">e0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_name</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
        <span class="n">freq_list_i</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">feature_name</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">][</span><span class="n">y_name</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
        <span class="n">e_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">freq_list_i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="o">*</span> <span class="n">entropy</span><span class="p">(</span><span class="n">freq_list_i</span><span class="p">)</span>
        <span class="n">info_gain</span> <span class="o">-=</span> <span class="n">e_i</span>
    <span class="k">return</span> <span class="n">info_gain</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Outlook&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="s1">&#39;humidity&#39;</span><span class="p">,</span> <span class="s1">&#39;windy&#39;</span><span class="p">]</span>
<span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">information_gain</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s1">&#39;play&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">features</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;Outlook&#39;: 0.24674981977443933,
 &#39;temperature&#39;: 0.02922256565895487,
 &#39;humidity&#39;: 0.15183550136234164,
 &#39;windy&#39;: 0.048127030408269544}</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="./img/playtree.jpg" alt=""></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>RQ</strong>: Guess what is the animal I am thinking?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The binary splitting makes this extremely efficient: in a well-constructed tree,</p>
<ul>
<li>each question will cut the number of options by approximately half, </li>
<li>very quickly narrowing the options even among a large number of classes.</li>
</ul>
<p><font color = 'purple'>The trick comes in deciding which questions to ask at each step.</font></p>
<p>Using axis-aligned splits in the data:</p>
<ul>
<li>each node in the tree splits the data into two groups using a cutoff value within one of the features.</li>
</ul>
<p>Let's now look at an example of this.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-decision-tree">Creating a decision tree<a class="anchor-link" href="#Creating-a-decision-tree"> </a></h3><p>Consider the following two-dimensional data, which has one of four class labels:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_28_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A simple decision tree built on this data will iteratively split the data along one or the other axis</p>
<ul>
<li>according to some quantitative criterion, and </li>
<li>at each level assign the label of the new region according to a majority vote of points within it.</li>
</ul>
<p>This figure presents a visualization of <strong>the first four levels</strong> of a decision tree classifier for this data:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="img/figures/05.08-decision-tree-levels.png" alt="">
<a href="06.00-Figure-Code.ipynb#Decision-Tree-Levels">figure source in Appendix</a></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Notice">Notice<a class="anchor-link" href="#Notice"> </a></h2><p>after the each split</p>
<ul>
<li>Nodes that contain all of one color will not be splitted again. </li>
<li>At each level <em>every</em> region is again split along one of the two features.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This process of fitting a decision tree to our data can be done in Scikit-Learn with the <code>DecisionTreeClassifier</code> estimator:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's write a quick utility function to help us visualize the output of the classifier:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="c1"># Plot the training points</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
               <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
    
    <span class="c1"># fit the estimator</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Create a color plot with the results</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">contours</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                           <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>
                           <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
                           <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can examine what the decision tree classification looks like:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">visualize_classifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_37_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you're running this notebook live, you can use the helpers script included in <a href="06.00-Figure-Code.ipynb#Helper-Code">The Online Appendix</a> to bring up an interactive visualization of the decision tree building process:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">plot_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/datalab/Applications/anaconda/lib/python3.5/site-packages/matplotlib/contour.py:1000: UserWarning: The following kwargs were not used by contour: &#39;clim&#39;
  s)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_39_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that as the depth increases, we tend to get very strangely shaped classification regions;</p>
<ul>
<li>for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.</li>
<li>It's clear that this is less a result of the true, intrinsic data distribution</li>
<li>It's more a result of the particular sampling or noise properties of the data.</li>
</ul>
<p>That is, this decision tree, even at only five levels deep, is clearly <strong>over-fitting</strong> our data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decision-trees-and-over-fitting">Decision trees and over-fitting<a class="anchor-link" href="#Decision-trees-and-over-fitting"> </a></h3><p>Such over-fitting turns out to be a general property of decision trees:</p>
<ul>
<li>it is very easy to go too deep in the tree<ul>
<li>to fit details of the particular data rather than the overall properties of the distributions they are drawn from.</li>
</ul>
</li>
</ul>
<p>Another way to see this over-fitting is</p>
<ul>
<li>to look at models trained on different subsets of the data</li>
</ul>
<p>for example, in this figure we train two different trees, each on half of the original data:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="img/figures/05.08-decision-tree-overfitting.png" alt="">
<a href="06.00-Figure-Code.ipynb#Decision-Tree-Overfitting">figure source in Appendix</a></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is clear that</p>
<ul>
<li>in some places, the two trees produce consistent results <ul>
<li>e.g., in the four corners</li>
</ul>
</li>
<li>while in other places, the two trees give very different classifications <ul>
<li>e.g., in the regions between any two clusters</li>
</ul>
</li>
</ul>
<p>The key observation is that the inconsistencies tend to happen where the classification is less certain,</p>
<blockquote><h3 id="by-using-information-from-both-of-these-trees,-we-might-come-up-with-a-better-result!">by using information from <em>both</em> of these trees, we might come up with a better result!<a class="anchor-link" href="#by-using-information-from-both-of-these-trees,-we-might-come-up-with-a-better-result!"> </a></h3>
</blockquote>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you are running this notebook live, the following function will allow you to interactively display the fits of trees trained on a random subset of the data:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">randomized_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/datalab/Applications/anaconda/lib/python3.5/site-packages/matplotlib/contour.py:1000: UserWarning: The following kwargs were not used by contour: &#39;clim&#39;
  s)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_45_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><font size = '3pt'>Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further.</font></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ensembles-of-Estimators:-Random-Forests">Ensembles of Estimators: Random Forests<a class="anchor-link" href="#Ensembles-of-Estimators:-Random-Forests"> </a></h2><p><font color = 'red'>Multiple overfitting estimators can be combined to reduce the effect of this overfitting.</font>
This notion is  called <strong>bagging</strong>.</p>
<ul>
<li><p>an ensemble method (é›†æˆå­¦ä¹ )</p>
</li>
<li><p>Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators,</p>
<ul>
<li>each of which over-fits the data, and </li>
<li>averages the results to find a better classification.</li>
</ul>
</li>
</ul>
<p>An ensemble of randomized decision trees is known as a <strong>random forest</strong>.</p>
<p>This type of bagging classification can be done manually using Scikit-Learn's <code>BaggingClassifier</code> meta-estimator, as shown here:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">BaggingClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">bag</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">bag</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">bag</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/datalab/Applications/anaconda/lib/python3.5/site-packages/matplotlib/contour.py:960: UserWarning: The following kwargs were not used by contour: &#39;clim&#39;
  s)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_48_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.</p>
<p>In practice, decision trees are more effectively randomized by <strong>injecting some stochasticity in how the splits are chosen</strong>:</p>
<ul>
<li>this way <strong>all the data contributes to the fit each time</strong></li>
<li>but the results of the fit still have the desired randomness.</li>
<li>when determining which feature to split on, the randomized tree might select from among the <strong>top several features</strong>.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can read more technical details about these randomization strategies in the <a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">Scikit-Learn documentation</a> and references within.</p>
<p>In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the <code>RandomForestClassifier</code> estimator, which takes care of all the randomization automatically.</p>
<p>All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/datalab/Applications/anaconda/lib/python3.5/site-packages/matplotlib/contour.py:960: UserWarning: The following kwargs were not used by contour: &#39;clim&#39;
  s)
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_51_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Random-Forest-Regression">Random Forest Regression<a class="anchor-link" href="#Random-Forest-Regression"> </a></h2><p>In the previous section we considered random forests within the context of classification.</p>
<p>Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables).</p>
<ul>
<li>The estimator to use for this is the <code>RandomForestRegressor</code>, and </li>
<li>the syntax is very similar to what we saw earlier.</li>
</ul>
<p>Consider the following data, drawn from the combination of a fast and slow oscillation:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="n">fast_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">slow_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">slow_oscillation</span> <span class="o">+</span> <span class="n">fast_oscillation</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_54_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the random forest regressor, we can find the best fit curve as follows:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">ytrue</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">,</span> <span class="s1">&#39;-r&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">ytrue</span><span class="p">,</span> <span class="s1">&#39;-k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_56_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.</p>
<p>As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example:-Random-Forest-for-Classifying-Digits">Example: Random Forest for Classifying Digits<a class="anchor-link" href="#Example:-Random-Forest-for-Classifying-Digits"> </a></h2><p>Earlier we took a quick look at the hand-written digits data (see <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a>).</p>
<p>Let's use that again here to see how the random forest classifier can be used in this context.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>dict_keys([&#39;images&#39;, &#39;data&#39;, &#39;DESCR&#39;, &#39;target_names&#39;, &#39;target&#39;])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To remind us what we're looking at, we'll visualize the first few data points:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># set up the figure</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  <span class="c1"># figure size in inches</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># plot the digits: each image is 8x8 pixels</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
    
    <span class="c1"># label the image with the target value</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_61_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can quickly classify the digits using a random forest as follows:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">ypred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can take a look at the classification report for this classifier:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>             precision    recall  f1-score   support

          0       1.00      0.97      0.99        38
          1       0.98      0.95      0.97        44
          2       0.95      1.00      0.98        42
          3       0.98      0.98      0.98        45
          4       0.97      1.00      0.99        37
          5       0.98      0.96      0.97        49
          6       1.00      1.00      1.00        52
          7       1.00      0.96      0.98        50
          8       0.94      0.98      0.96        46
          9       0.98      0.98      0.98        47

avg / total       0.98      0.98      0.98       450

</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="img/figures/precision_recall_f1.png" alt=""></p>
<ul>
<li><p>Precison æŸ¥å‡†ç‡</p>
<ul>
<li>æ£€ç´¢å‡ºæ¥çš„ä¿¡æ¯æœ‰å¤šå°‘æ˜¯ç”¨æˆ·æ„Ÿå…´è¶£çš„ï¼Ÿ
$$ P = \frac{TP}{TP + FP}$$</li>
</ul>
</li>
<li><p>Recall æŸ¥å…¨ç‡</p>
<ul>
<li>ç”¨æˆ·æ„Ÿå…´è¶£çš„ä¿¡æ¯æœ‰å¤šå°‘è¢«æ£€ç´¢å‡ºæ¥äº†ï¼Ÿ
$$ R = \frac{TP}{TP + FN}$$</li>
</ul>
</li>
<li><p>F1å¾—åˆ†</p>
<ul>
<li>æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡çš„è°ƒå’Œå¹³å‡æ•°
$$F1 = \frac{2 P R}{P+R}$$</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And for good measure, plot the confusion matrix:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;true label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;predicted label&#39;</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/09-08-random-forests_68_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We find that a simple, untuned random forest results in a very accurate classification of the digits data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary-of-Random-Forests">Summary of Random Forests<a class="anchor-link" href="#Summary-of-Random-Forests"> </a></h2><p>This section contained a brief introduction to the concept of <em>ensemble estimators</em>, and in particular the random forest â€“ an ensemble of randomized decision trees.
Random forests are a powerful method with several advantages:</p>
<ul>
<li>Both training and prediction are very fast, because of the simplicity of the underlying decision trees. <ul>
<li>In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities.</li>
</ul>
</li>
<li>The multiple trees allow for a probabilistic classification: <ul>
<li>a majority vote among estimators gives an estimate of the probability (accessed in Scikit-Learn with the <code>predict_proba()</code> method).</li>
</ul>
</li>
<li>The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.</li>
</ul>
<p>A primary disadvantage of random forests is that the results are not easily interpretable:</p>
<ul>
<li>if you would like to draw conclusions about the <em>meaning</em> of the classification model, random forests may not be the best choice.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><!--NAVIGATION-->
&lt; <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> | <a href="Index.ipynb">Contents</a> | <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> &gt;</p>

</div>
</div>
</div>
</div>

 


</main>
